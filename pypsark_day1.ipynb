{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ae1c62-e1f2-4aa8-8ec8-27bf5ac5b67c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "\n",
    "spark.session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77aac4a0-3d2d-4de3-8e5a-91328a40c6cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "categories = ['Electronics', 'Books', 'Clothing', 'Home', 'Toys']\n",
    "payment_methods = ['Credit Card', 'Debit Card', 'Gift Card', 'Net Banking', 'UPI']\n",
    "cities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']\n",
    "states = ['NY', 'CA', 'IL', 'TX', 'AZ']\n",
    "customer_names = ['John Doe', 'Jane Smith', 'Alice Johnson', 'Robert Brown', 'Emily Davis']\n",
    "\n",
    "def random_date():\n",
    "    start = datetime.date(2024, 1, 1)\n",
    "    end = datetime.date(2024, 12, 31)\n",
    "    return start + datetime.timedelta(days=random.randint(0, (end - start).days))\n",
    "\n",
    "def random_address():\n",
    "    return f\"{random.randint(100, 9999)} {random.choice(['Main St', '2nd St', '3rd St', '4th St', '5th St'])}, {random.choice(cities)}, {random.choice(states)}\"\n",
    "\n",
    "unique_rows = []\n",
    "for i in range(1, 21):\n",
    "    customer_id = f\"CUST{random.randint(1000, 9999)}\"\n",
    "    row = Row(\n",
    "        order_id=f\"ORD{i:04d}\",\n",
    "        customer_id=customer_id,\n",
    "        customer_name=random.choice(customer_names),\n",
    "        address=random_address(),\n",
    "        product_id=f\"PROD{random.randint(100, 999)}\",\n",
    "        category=random.choice(categories),\n",
    "        order_date=str(random_date()),\n",
    "        quantity=random.randint(1, 5),\n",
    "        price=round(random.uniform(10, 500), 2),\n",
    "        payment_method=random.choice(payment_methods),\n",
    "        city=random.choice(cities),\n",
    "        state=random.choice(states)\n",
    "    )\n",
    "    unique_rows.append(row)\n",
    "\n",
    "# 5 duplicate rows (pick from unique_rows)\n",
    "duplicate_rows = [unique_rows[i] for i in random.sample(range(20), 5)]\n",
    "\n",
    "# 5 rows with missing values in 'price'\n",
    "missing_rows = []\n",
    "for i in range(21, 26):\n",
    "    customer_id = f\"CUST{random.randint(1000, 9999)}\"\n",
    "    row = Row(\n",
    "        order_id=f\"ORD{i:04d}\",\n",
    "        customer_id=customer_id,\n",
    "        customer_name=random.choice(customer_names),\n",
    "        address=random_address(),\n",
    "        product_id=f\"PROD{random.randint(100, 999)}\",\n",
    "        category=random.choice(categories),\n",
    "        order_date=str(random_date()),\n",
    "        quantity=random.randint(1, 5),\n",
    "        price=None,\n",
    "        payment_method=random.choice(payment_methods),\n",
    "        city=random.choice(cities),\n",
    "        state=random.choice(states)\n",
    "    )\n",
    "    missing_rows.append(row)\n",
    "\n",
    "all_rows = unique_rows + duplicate_rows + missing_rows\n",
    "\n",
    "df_sales = spark.createDataFrame(all_rows)\n",
    "display(df_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c830f3-42ad-4e3e-a4b9-597014aff729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sales.display()\n",
    "df_sales.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b286a1a1-2e3b-4627-bd6b-9f4e8ee680dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True)\n",
    "])\n",
    "\n",
    "sales_raw_data = spark.createDataFrame(all_rows, schema)\n",
    "\n",
    "display(sales_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b7cc010-7f8e-48b4-a713-fd430a3c2b77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_raw_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e7af57-96f5-4a6c-82aa-1769f4c5e0d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show catalogs\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8562884c-474c-4c2d-989e-b9e52103cd52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"create catalog if not exists lakehouse\")\n",
    "spark.sql(\"show catalogs\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ca6fa4a-3193-4647-922f-8bd1a0b85950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE CATALOG lakehouse\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "008624d3-67c8-4c00-b520-94e9abcde32d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show schemas in lakehouse\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d47e3bb2-6bac-4dac-9570-79f77424046d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"create schema if not exists lakehouse.dev\")\n",
    "spark.sql(\"show schemas in lakehouse\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "439099d5-dc52-4cdb-9130-56cfb2e28727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE SCHEMA lakehouse.dev\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "573e47e1-f48b-4d3f-8fa7-4bef2c3c7504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_raw_data.write.mode(\"overwrite\").saveAsTable(\"lakehouse.dev.sales_raw_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "691b100a-ce3b-4db3-9b62-7a88e175be89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from lakehouse.dev.sales_raw_data\").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd0bfae-76ec-49ea-8107-9b14da19ff7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED lakehouse.dev.sales_raw_data\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cff8e49-2985-425a-8bf7-e001a2f01b3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Normal tables in Databricks are typically managed tables stored in the default data source (e.g., Hive).\n",
    "# Delta tables in Databricks are managed tables that use the Delta Lake format, providing ACID transactions, scalable metadata handling, and data versioning.\n",
    "\n",
    "# Example of creating a normal table using Delta format\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS lakehouse.dev.normal_table (\n",
    "  id INT,\n",
    "  name STRING,\n",
    "  value DOUBLE\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Inserting data into the normal table\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO lakehouse.dev.normal_table VALUES\n",
    "(1, 'Alice', 10.5),\n",
    "(2, 'Bob', 20.0),\n",
    "(3, 'Charlie', 30.5)\n",
    "\"\"\")\n",
    "\n",
    "# Example of creating a Delta table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS lakehouse.dev.delta_table (\n",
    "  id INT,\n",
    "  name STRING,\n",
    "  value DOUBLE\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Inserting data into the Delta table\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO lakehouse.dev.delta_table VALUES\n",
    "(1, 'Alice', 10.5),\n",
    "(2, 'Bob', 20.0),\n",
    "(3, 'Charlie', 30.5)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2969f206-3a90-41dd-88a9-ff5a1dcc041a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from lakehouse.dev.normal_table\").display()\n",
    "spark.sql(\"select * from lakehouse.dev.delta_table\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf93bda-3719-4a5a-953a-2eb88dfbefc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED lakehouse.dev.normal_table\").display()\n",
    "spark.sql(\"DESCRIBE EXTENDED lakehouse.dev.delta_table\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e74aa3c3-7ee6-424b-a65e-bb3509b2d1ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def is_delta_table(table_name):\n",
    "    details = spark.sql(f\"DESCRIBE EXTENDED {table_name}\").collect()\n",
    "    for row in details:\n",
    "        if row.col_name == 'Provider' and row.data_type == 'delta':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "table_name = \"lakehouse.dev.normal_table\"\n",
    "if is_delta_table(table_name):\n",
    "    print(f\"{table_name} is a Delta table.\")\n",
    "else:\n",
    "    print(f\"{table_name} is a normal table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa51952f-eef2-44bd-aa25-644b5351434d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_raw_data.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"lakehouse.dev.sales_delta\")\n",
    "spark.sql(\"select * from lakehouse.dev.sales_delta\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab4cfe6a-a280-4191-a862-37ecec20c9a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show tables in lakehouse.dev\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba0fffc5-465e-45c7-a570-24aa808b521d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Load the sales_delta table into a DataFrame\n",
    "sales_df = spark.table(\"lakehouse.dev.sales_delta\")\n",
    "\n",
    "# Example of an inner join\n",
    "inner_join_df = sales_df.alias(\"a\").join(sales_df.alias(\"b\"), col(\"a.order_id\") == col(\"b.order_id\"), \"inner\")\n",
    "display(inner_join_df)\n",
    "\n",
    "# Example of a left join\n",
    "left_join_df = sales_df.alias(\"a\").join(sales_df.alias(\"b\"), col(\"a.order_id\") == col(\"b.order_id\"), \"left\")\n",
    "display(left_join_df)\n",
    "\n",
    "# Example of a right join\n",
    "right_join_df = sales_df.alias(\"a\").join(sales_df.alias(\"b\"), col(\"a.order_id\") == col(\"b.order_id\"), \"right\")\n",
    "display(right_join_df)\n",
    "\n",
    "# Example of a full outer join\n",
    "full_outer_join_df = sales_df.alias(\"a\").join(sales_df.alias(\"b\"), col(\"a.order_id\") == col(\"b.order_id\"), \"outer\")\n",
    "display(full_outer_join_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55a54093-b4fb-4673-8f1c-58f753cd095e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "creating two tables and doing operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a214db-fd4b-43e3-afb8-72aa330da27f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE CATALOG IF NOT EXISTS practice\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS practice.dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8015343a-8885-44ba-a405-f7f885031255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show tables in practice.dev\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "180738cc-2eb4-492d-825e-612fe03720f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS practice.dev.sales (\n",
    "        sale_id INT,\n",
    "        product STRING,\n",
    "        amount DOUBLE,\n",
    "        order_id INT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO practice.dev.sales VALUES\n",
    "    (1, 'Product A', 100.0, 1),\n",
    "    (2, 'Product B', 150.0, 2),\n",
    "    (3, 'Product C', 200.0, 3),\n",
    "    (4, 'Product D', 250.0, NULL),\n",
    "    (1, 'Product A', 100.0, 1)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS practice.dev.orders (\n",
    "        order_id INT,\n",
    "        customer STRING,\n",
    "        order_date DATE\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO practice.dev.orders VALUES\n",
    "    (1, 'Customer X', '2025-06-25'),\n",
    "    (2, 'Customer Y', '2025-06-26'),\n",
    "    (3, 'Customer Z', '2025-06-27'),\n",
    "    (NULL, 'Customer W', '2025-06-28'),\n",
    "    (1, 'Customer X', '2025-06-25')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2732f6fa-1225-46a1-a2fa-15e7562d6394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from practice.dev.orders;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5198aa8-ea8e-4cb5-81be-dcea810b97f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from practice.dev.sales;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5996ff94-f937-4b13-9722-09b6fcb32848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First drop duplicates from both tables by creating df for both tables\n",
    "\n",
    "df_sales = spark.table(\"practice.dev.sales\")\n",
    "df_orders = spark.table(\"practice.dev.orders\")\n",
    "\n",
    "# Drop duplicates \n",
    "df_sales_dedup = df_sales.dropDuplicates()\n",
    "df_orders_dedup = df_orders.dropDuplicates()\n",
    "\n",
    "# Display unique records\n",
    "df_sales_dedup.sort(\"sale_id\", ascending=True).display()\n",
    "df_orders_dedup.sort(\"order_id\", ascending=True).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "395e2875-23dc-480b-9001-a6bf3ee901f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Join the deduplicated sales and orders dataframes\n",
    "df_joined = df_sales_dedup.join(\n",
    "    df_orders_dedup,\n",
    "    df_sales_dedup.order_id == df_orders_dedup.order_id,\n",
    "    \"inner\"\n",
    ")\n",
    "df_joined.orderBy(\"orders.order_id\", ascending=True).display()\n",
    "#df_joined.sort(\"sale_id\", ascending=True).display()\n",
    "\n",
    "\n",
    "# Group by product and calculate the total sales amount for each product\n",
    "df_product_sales = df_joined.groupBy(\"product\").agg(sum(\"amount\").alias(\"total_sales\"))\n",
    "display(df_product_sales)\n",
    "\n",
    "# Find the product with the highest sales\n",
    "df_highest_sales_product = df_product_sales.orderBy(\"total_sales\", ascending=False).limit(1)\n",
    "\n",
    "# Display the product with the highest sales\n",
    "display(df_highest_sales_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3f9855c-984b-4880-9f0a-fac825f6feaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "# Define a window specification\n",
    "window_spec = Window.orderBy(\"sale_id\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Calculate the running total of sales\n",
    "df_running_total = df_joined.withColumn(\"running_total\", spark_sum(\"amount\").over(window_spec))\n",
    "\n",
    "# Display the running total of sales\n",
    "display(df_running_total)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4815915837870727,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pypsark_day1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
